{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.039 Deep Learning Project\n",
    "\n",
    "Group Members:\n",
    "- Lee Chang Zheng\n",
    "- Lee Cheng Xin\n",
    "- Jason Peng Jing Ming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2482de17af0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 12\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Audio samples into Spectrograms\n",
    "\n",
    "Before we can begin, we first have to convert the audio samples from .webm format into a standardised format. We will convert the files to .wav with single channel, a sample rate of 48000 Hz, and pad the audio files to 10 seconds long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, time\n",
    "class ProcessingPipeline:\n",
    "    def __init__(self, processes):\n",
    "        self.processes = processes\n",
    "\n",
    "    def run_pipeline(self,input_folder, output_folder):\n",
    "        self.delete_files(output_folder)\n",
    "        if len(self.processes) == 1:\n",
    "            process = self.processes[0]\n",
    "            process(input_folder, output_folder)\n",
    "            return\n",
    "        if not os.path.exists(\"tmp1\"):\n",
    "            os.makedirs(\"tmp1\")\n",
    "        if not os.path.exists(\"tmp2\"):\n",
    "            os.makedirs(\"tmp2\")\n",
    "        \n",
    "        curr_folder = \"./tmp1\"\n",
    "        next_folder = \"./tmp2\"\n",
    "        process = self.processes[0]\n",
    "        print(input_folder, curr_folder)\n",
    "        print(process.__name__)\n",
    "        process(input_folder, curr_folder)\n",
    "        for i in range(1, len(self.processes) - 1):\n",
    "            process = self.processes[i]\n",
    "            self.delete_files(next_folder)\n",
    "            #swap folders\n",
    "            print(curr_folder, next_folder)\n",
    "            print(process.__name__)\n",
    "            process(curr_folder, next_folder)\n",
    "            a = curr_folder\n",
    "            curr_folder = next_folder\n",
    "            next_folder = a\n",
    "        process = self.processes[-1]\n",
    "        print(\"last\")\n",
    "        print(process.__name__)\n",
    "        print(curr_folder, output_folder)\n",
    "        process(curr_folder, output_folder)\n",
    "        self.delete_files(\"./tmp1\")\n",
    "        self.delete_files(\"./tmp2\")\n",
    "        os.rmdir(\"./tmp1\")\n",
    "        os.rmdir(\"./tmp2\")\n",
    "\n",
    "    def delete_files(self, folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print('Failed to delete %s. Reason: %s' % (file_path, e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert webm and ogg files to wav with single channel, sample rate of 48000 Hz, padded to 10 seconds long\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def convert_audio(in_path, out_path):\n",
    "    if '.webm' in in_path:\n",
    "        subprocess.run([\"ffmpeg\", \"-i\", in_path, \"-y\", \"-ac\", \"1\", \"-ar\", \"48000\", out_path.replace('.webm', '.wav')])\n",
    "    elif '.ogg' in in_path:\n",
    "        subprocess.run([\"ffmpeg\", \"-i\", in_path, \"-y\", \"-ac\", \"1\", \"-ar\", \"48000\", out_path.replace('.ogg', '.wav')])\n",
    "    elif '.wav' in in_path:\n",
    "        subprocess.run([\"ffmpeg\", \"-i\", in_path, \"-y\", \"-ac\", \"1\", \"-ar\", \"48000\", out_path.replace('.wav', '.wav')])\n",
    "\n",
    "def pad_trim_audio(in_path, out_path):\n",
    "    audio = AudioSegment.from_wav(in_path)\n",
    "    if len(audio) < 2500:\n",
    "        padding = AudioSegment.silent(duration=2500 - len(audio))\n",
    "        padded_audio = audio + padding\n",
    "        padded_audio.export(out_path, format='wav')\n",
    "    else:\n",
    "        trimmed_audio = audio[:2500]\n",
    "        trimmed_audio.export(out_path, format='wav')\n",
    "    \n",
    "# Note: These are commented out as the conversion has been done, it is simply for reference. You will need FFmpeg to run this.\n",
    "# for filename in os.listdir('./Data/Covid'):\n",
    "#     convert_audio(f'./Data/Covid/{filename}', f'./Converted/{filename}')\n",
    "# for filename in os.listdir('./Data/Healthy'):\n",
    "#     convert_audio(f'./Data/Healthy/{filename}', f'./Converted/{filename}')\n",
    "\n",
    "# # Padding/trimming the audio to 10 seconds long\n",
    "# for filename in os.listdir('./Converted'):\n",
    "#     pad_trim_audio(f'./Converted/{filename}', f'./Converted/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Inputs ./tmp1\n",
      "convert_audio_folder\n",
      "./tmp1 ./tmp2\n",
      "noise_reduce\n",
      "Number 0\n",
      "Number 1\n",
      "./tmp2 ./tmp1\n",
      "trim_silence\n",
      "00a0156b-7179-4773-8a2c-4bb919e076bd.wav\n",
      "0\n",
      "00bf9f83-2e8f-47cf-a4f2-97f2beceebc1.wav\n",
      "1\n",
      "last\n",
      "pad_folder\n",
      "./tmp1 ./Outputs\n"
     ]
    }
   ],
   "source": [
    "from noise_reduction import noise_reduce\n",
    "from silence import trim_silence\n",
    "def convert_audio_folder(input_folder, output_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        convert_audio(f'{input_folder}/{filename}', f'{output_folder}/{filename}')\n",
    "\n",
    "def pad_folder(input_folder, output_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        pad_trim_audio(f'{input_folder}/{filename}', f'{output_folder}/{filename}')\n",
    "\n",
    "pipeline_functions = [convert_audio_folder, noise_reduce, trim_silence, pad_folder ]\n",
    "\n",
    "covid_pipeline = ProcessingPipeline(pipeline_functions)\n",
    "covid_pipeline.run_pipeline(\"./Inputs\", \"./Outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardizing the audio samples, we need to convert them into a Mel Spectrogram for the CNN model to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the audio waveform into a spectrogram\n",
    "def audio_to_spec(audio, sample_rate, n_mels=128, n_fft=400, win_length=None, hop_length=None, top_db=80):\n",
    "    mel_spectrogram = transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        n_mels=n_mels,\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(audio)\n",
    "    \n",
    "    transform = transforms.AmplitudeToDB(top_db=top_db)\n",
    "    final_spec = transform(melspec)\n",
    "    return final_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the spectrogram using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and Dataloader\n",
    "\n",
    "We create a custom dataset for the model. The custom dataset will convert the audio into a spectrogram before feeding it into the model. It will also ensure that the dimensions of the spectrograms are consistent, trimming and padding any spectrogram image that is too long or too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidCoughDataset(Dataset):\n",
    "    def __init__(self, filename, datapath):\n",
    "        df = pd.read_excel(filename)\n",
    "        self.df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        self.datapath = datapath\n",
    "        self.max_spec_length = 2400\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_file = self.datapath + self.df.iloc[index, 0] + '.wav'\n",
    "        status = 1 if self.df.iloc[index, 1] == 'COVID' else 0        \n",
    "        audio, sample_rate = torchaudio.load(audio_file)\n",
    "        spec = audio_to_spec(audio=audio, sample_rate=sample_rate)\n",
    "        \n",
    "        # Note: There might be some minor differences in the length of the audio clips, resulting in spectrograms of different\n",
    "        #       dimensions. We need to pad/trim the spectrograms to ensure consistency before we can feed into the model. \n",
    "        # Pad the shorter spectrograms to the maximum length\n",
    "        if spec.shape[2] < self.max_spec_length:\n",
    "            spec = F.pad(spec, (0, self.max_spec_length - spec.shape[2]), value=0)\n",
    "        # Trim the longer spectrograms to the maximum length\n",
    "        elif spec.shape[2] > self.max_spec_length:\n",
    "            spec = spec[:, :, :self.max_spec_length]\n",
    "        return spec, status\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.df)\n",
    "    \n",
    "    def visual_spectrogram(self,index):\n",
    "        audio_file = self.datapath + self.df.iloc[index, 0] + '.wav'\n",
    "        # status = 1 if self.df.iloc[index, 1] == 'COVID' else 0        \n",
    "        audio, sample_rate = torchaudio.load(audio_file)\n",
    "        spec = audio_to_spec(audio=audio, sample_rate=sample_rate)\n",
    "        if spec.shape[2] < self.max_spec_length:\n",
    "            spec = F.pad(spec, (0, self.max_spec_length - spec.shape[2]), value=0)\n",
    "        # Trim the longer spectrograms to the maximum length\n",
    "        elif spec.shape[2] > self.max_spec_length:\n",
    "            spec = spec[:, :, :self.max_spec_length]\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1)\n",
    "        plot_waveform(audio, sample_rate, title=\"Original waveform\", ax=axs[0])\n",
    "        plot_spectrogram(spec[0], title=\"spectrogram\", ax=axs[1])\n",
    "        fig.tight_layout()\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "def plot_fbank(fbank, title=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Filter bank\")\n",
    "    axs.imshow(fbank, aspect=\"auto\")\n",
    "    axs.set_ylabel(\"frequency bin\")\n",
    "    axs.set_xlabel(\"mel bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening './Outputs/0be065d1-12d2-4ccc-8d76-21282c8c47e9.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CovidCoughDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/Dataset.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Outputs/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mvisual_spectrogram(\u001b[38;5;241m300\u001b[39m))\n\u001b[0;32m      7\u001b[0m train_dataset, valid_dataset, test_dataset \u001b[38;5;241m=\u001b[39m random_split(dataset, [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mCovidCoughDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      9\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapath \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOVID\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m        \n\u001b[1;32m---> 11\u001b[0m audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m spec \u001b[38;5;241m=\u001b[39m audio_to_spec(audio\u001b[38;5;241m=\u001b[39maudio, sample_rate\u001b[38;5;241m=\u001b[39msample_rate)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Note: There might be some minor differences in the length of the audio clips, resulting in spectrograms of different\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#       dimensions. We need to pad/trim the spectrograms to ensure consistency before we can feed into the model. \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Pad the shorter spectrograms to the maximum length\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchaudio\\backend\\soundfile_backend.py:221\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    141\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msoundfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWAV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m normalize:\n\u001b[0;32m    223\u001b[0m             dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening './Outputs/0be065d1-12d2-4ccc-8d76-21282c8c47e9.wav': System error."
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "batch_size = 32\n",
    "\n",
    "dataset = CovidCoughDataset('./Data/Dataset.xlsx', './Outputs/')\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset.visual_spectrogram(300))\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidClassifer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Note: Each spectrogram is 128 x 2400. We divide by 9 as there are two 3 by 3 kernels with stride 1 and padding 1. \n",
    "        self.fc1 = nn.Linear(64 * (128//27) * (2400//27), 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flattens the 2D images into 1D\n",
    "        x = x.view(-1, 64 * (128//27) * (2400//27))\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize epoch loss and accuracy\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "    # Calculate epoch loss and accuracy\n",
    "    total_loss /= len(test_dataloader)\n",
    "    accuracy = correct/total\n",
    "    \n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, epochs = 10, lr = 0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Initialize epoch loss and accuracy\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_number, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}, Batch number: {batch_number}, Cumulated accuracy: {correct/total}')\n",
    "        \n",
    "        # Calculate epoch loss and accuracy\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        epoch_acc = correct/total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        print(f'--- Epoch {epoch+1}/{epochs}: Train loss: {epoch_loss:.4f}, Train accuracy: {epoch_acc:.4f}')\n",
    "    \n",
    "        epoch_loss, epoch_acc = evaluate(model, valid_dataloader)    \n",
    "        valid_losses.append(epoch_loss)\n",
    "        valid_accuracies.append(epoch_acc)\n",
    "        print(f'--- Epoch {epoch+1}/{epochs}: Validation loss: {epoch_loss:.4f}, Validation accuracy: {epoch_acc:.4f}')            \n",
    "    \n",
    "    return train_losses, train_accuracies, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CovidClassifer().to(device)\n",
    "\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies = train(model, train_dataloader, valid_dataloader, epochs = 3, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(train_losses, \"r-\", label = \"Train losses\")\n",
    "plt.plot(valid_losses, \"b-\", label = \"Validation losses\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(train_accuracies, \"r-\", label = \"Train accuracy\")\n",
    "plt.plot(valid_accuracies, \"b-\", label = \"Validation accuracy\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate(model, test_dataloader)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
