{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.039 Deep Learning Project\n",
    "\n",
    "Group Members:\n",
    "- Lee Chang Zheng\n",
    "- Lee Cheng Xin\n",
    "- Jason Peng Jing Ming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x241b83a6070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 12\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Audio samples into Spectrograms\n",
    "\n",
    "Before we can begin, we first have to convert the audio samples from .webm format into a standardised format. We will convert the files to .wav with single channel, a sample rate of 48000 Hz, and pad the audio files to 10 seconds long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert webm and ogg files to wav with single channel, sample rate of 48000 Hz, padded to 10 seconds long\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def convert_audio(in_path, out_path):\n",
    "    if '.webm' in in_path:\n",
    "        subprocess.run([\"ffmpeg\", \"-i\", in_path, \"-y\", \"-ac\", \"1\", \"-ar\", \"48000\", out_path.replace('.webm', '.wav')])\n",
    "    elif '.ogg' in in_path:\n",
    "        subprocess.run([\"ffmpeg\", \"-i\", in_path, \"-y\", \"-ac\", \"1\", \"-ar\", \"48000\", out_path.replace('.ogg', '.wav')])\n",
    "    elif '.wav' in in_path:\n",
    "        subprocess.run([\"ffmpeg\", \"-i\", in_path, \"-y\", \"-ac\", \"1\", \"-ar\", \"48000\", out_path])\n",
    "\n",
    "def pad_trim_audio(in_path, out_path):\n",
    "    audio = AudioSegment.from_wav(in_path)\n",
    "    if len(audio) < 10000:\n",
    "        padding = AudioSegment.silent(duration=10000 - len(audio))\n",
    "        padded_audio = audio + padding\n",
    "        padded_audio.export(out_path, format='wav')\n",
    "    elif len(audio) > 10000:\n",
    "        trimmed_audio = audio[:10000]\n",
    "        trimmed_audio.export(out_path, format='wav')\n",
    "    \n",
    "# Note: These are commented out as the conversion has been done, it is simply for reference. You will need FFmpeg to run this.\n",
    "# for filename in os.listdir('./Data/Covid'):\n",
    "#     convert_audio(f'./Data/Covid/{filename}', f'./Converted/{filename}')\n",
    "# for filename in os.listdir('./Data/Healthy'):\n",
    "#     convert_audio(f'./Data/Healthy/{filename}', f'./Converted/{filename}')\n",
    "\n",
    "# # Padding/trimming the audio to 10 seconds long\n",
    "# for filename in os.listdir('./Converted'):\n",
    "#     pad_trim_audio(f'./Converted/{filename}', f'./Converted/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardizing the audio samples, we need to convert them into a Mel Spectrogram for the CNN model to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the audio waveform into a spectrogram\n",
    "def audio_to_spec(audio, sample_rate, n_mels=128, n_fft=400, win_length=None, hop_length=None, top_db=80):\n",
    "    mel_spectrogram = transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        n_mels=n_mels,\n",
    "    )\n",
    "    \n",
    "    melspec = mel_spectrogram(audio)\n",
    "    \n",
    "    transform = transforms.AmplitudeToDB(top_db=top_db)\n",
    "    final_spec = transform(melspec)\n",
    "    return final_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and Dataloader\n",
    "\n",
    "We create a custom dataset for the model. The custom dataset will convert the audio into a spectrogram before feeding it into the model. It will also ensure that the dimensions of the spectrograms are consistent, trimming and padding any spectrogram image that is too long or too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidCoughDataset(Dataset):\n",
    "    def __init__(self, filename, datapath):\n",
    "        df = pd.read_excel(filename)\n",
    "        self.df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        self.datapath = datapath\n",
    "        self.max_spec_length = 2400\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_file = self.datapath + self.df.iloc[index, 0] + '.wav'\n",
    "        status = 1 if self.df.iloc[index, 1] == 'COVID' else 0        \n",
    "        audio, sample_rate = torchaudio.load(audio_file)\n",
    "        spec = audio_to_spec(audio=audio, sample_rate=sample_rate)\n",
    "        \n",
    "        # Note: There might be some minor differences in the length of the audio clips, resulting in spectrograms of different\n",
    "        #       dimensions. We need to pad/trim the spectrograms to ensure consistency before we can feed into the model. \n",
    "        # Pad the shorter spectrograms to the maximum length\n",
    "        if spec.shape[2] < self.max_spec_length:\n",
    "            spec = F.pad(spec, (0, self.max_spec_length - spec.shape[2]), value=0)\n",
    "        # Trim the longer spectrograms to the maximum length\n",
    "        elif spec.shape[2] > self.max_spec_length:\n",
    "            spec = spec[:, :, :self.max_spec_length]\n",
    "        return spec, status\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 2400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\dlproject\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "batch_size = 32\n",
    "\n",
    "dataset = CovidCoughDataset('./Data/Dataset.xlsx', './Converted/')\n",
    "print(dataset[0][0].shape)\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidClassifer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Note: Each spectrogram is 128 x 2400. We divide by 9 as there are two 3 by 3 kernels with stride 1 and padding 1. \n",
    "        self.fc1 = nn.Linear(64 * (128//27) * (2400//27), 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flattens the 2D images into 1D\n",
    "        x = x.view(-1, 64 * (128//27) * (2400//27))\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize epoch loss and accuracy\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "    # Calculate epoch loss and accuracy\n",
    "    total_loss /= len(test_dataloader)\n",
    "    accuracy = correct/total\n",
    "    \n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, epochs = 10, lr = 0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Initialize epoch loss and accuracy\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_number, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}, Batch number: {batch_number}, Cumulated accuracy: {correct/total}')\n",
    "        \n",
    "        # Calculate epoch loss and accuracy\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        epoch_acc = correct/total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        print(f'--- Epoch {epoch+1}/{epochs}: Train loss: {epoch_loss:.4f}, Train accuracy: {epoch_acc:.4f}')\n",
    "    \n",
    "        epoch_loss, epoch_acc = evaluate(model, valid_dataloader)    \n",
    "        valid_losses.append(epoch_loss)\n",
    "        valid_accuracies.append(epoch_acc)\n",
    "        print(f'--- Epoch {epoch+1}/{epochs}: Validation loss: {epoch_loss:.4f}, Validation accuracy: {epoch_acc:.4f}')            \n",
    "    \n",
    "    return train_losses, train_accuracies, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch number: 0, Cumulated accuracy: 0.59375\n",
      "Epoch 1/3, Batch number: 1, Cumulated accuracy: 0.578125\n",
      "Epoch 1/3, Batch number: 2, Cumulated accuracy: 0.5729166666666666\n",
      "Epoch 1/3, Batch number: 3, Cumulated accuracy: 0.53125\n",
      "Epoch 1/3, Batch number: 4, Cumulated accuracy: 0.5125\n",
      "Epoch 1/3, Batch number: 5, Cumulated accuracy: 0.5208333333333334\n",
      "Epoch 1/3, Batch number: 6, Cumulated accuracy: 0.53125\n",
      "Epoch 1/3, Batch number: 7, Cumulated accuracy: 0.5234375\n",
      "Epoch 1/3, Batch number: 8, Cumulated accuracy: 0.5347222222222222\n",
      "Epoch 1/3, Batch number: 9, Cumulated accuracy: 0.53125\n",
      "Epoch 1/3, Batch number: 10, Cumulated accuracy: 0.5284090909090909\n",
      "Epoch 1/3, Batch number: 11, Cumulated accuracy: 0.5390625\n",
      "Epoch 1/3, Batch number: 12, Cumulated accuracy: 0.5408653846153846\n",
      "Epoch 1/3, Batch number: 13, Cumulated accuracy: 0.5424107142857143\n",
      "Epoch 1/3, Batch number: 14, Cumulated accuracy: 0.5416666666666666\n",
      "Epoch 1/3, Batch number: 15, Cumulated accuracy: 0.54296875\n",
      "Epoch 1/3, Batch number: 16, Cumulated accuracy: 0.5422794117647058\n",
      "Epoch 1/3, Batch number: 17, Cumulated accuracy: 0.5434027777777778\n",
      "Epoch 1/3, Batch number: 18, Cumulated accuracy: 0.5411184210526315\n",
      "Epoch 1/3, Batch number: 19, Cumulated accuracy: 0.5359375\n",
      "Epoch 1/3, Batch number: 20, Cumulated accuracy: 0.5327380952380952\n",
      "Epoch 1/3, Batch number: 21, Cumulated accuracy: 0.5326704545454546\n",
      "Epoch 1/3, Batch number: 22, Cumulated accuracy: 0.529891304347826\n",
      "Epoch 1/3, Batch number: 23, Cumulated accuracy: 0.5338541666666666\n",
      "Epoch 1/3, Batch number: 24, Cumulated accuracy: 0.53125\n",
      "Epoch 1/3, Batch number: 25, Cumulated accuracy: 0.5336538461538461\n",
      "Epoch 1/3, Batch number: 26, Cumulated accuracy: 0.5347222222222222\n",
      "Epoch 1/3, Batch number: 27, Cumulated accuracy: 0.5357142857142857\n",
      "Epoch 1/3, Batch number: 28, Cumulated accuracy: 0.5366379310344828\n",
      "Epoch 1/3, Batch number: 29, Cumulated accuracy: 0.5354166666666667\n",
      "Epoch 1/3, Batch number: 30, Cumulated accuracy: 0.5383064516129032\n",
      "Epoch 1/3, Batch number: 31, Cumulated accuracy: 0.537109375\n",
      "Epoch 1/3, Batch number: 32, Cumulated accuracy: 0.5350378787878788\n",
      "Epoch 1/3, Batch number: 33, Cumulated accuracy: 0.5367647058823529\n",
      "Epoch 1/3, Batch number: 34, Cumulated accuracy: 0.5375\n",
      "Epoch 1/3, Batch number: 35, Cumulated accuracy: 0.5399305555555556\n",
      "Epoch 1/3, Batch number: 36, Cumulated accuracy: 0.5396959459459459\n",
      "Epoch 1/3, Batch number: 37, Cumulated accuracy: 0.5394736842105263\n",
      "Epoch 1/3, Batch number: 38, Cumulated accuracy: 0.5392628205128205\n",
      "Epoch 1/3, Batch number: 39, Cumulated accuracy: 0.54140625\n",
      "Epoch 1/3, Batch number: 40, Cumulated accuracy: 0.5365853658536586\n",
      "Epoch 1/3, Batch number: 41, Cumulated accuracy: 0.5360668185269551\n",
      "--- Epoch 1/3: Train loss: 1.4768, Train accuracy: 0.5361\n",
      "--- Epoch 1/3: Validation loss: 0.6992, Validation accuracy: 0.4939\n",
      "Epoch 2/3, Batch number: 0, Cumulated accuracy: 0.6875\n",
      "Epoch 2/3, Batch number: 1, Cumulated accuracy: 0.578125\n",
      "Epoch 2/3, Batch number: 2, Cumulated accuracy: 0.625\n",
      "Epoch 2/3, Batch number: 3, Cumulated accuracy: 0.6171875\n",
      "Epoch 2/3, Batch number: 4, Cumulated accuracy: 0.60625\n",
      "Epoch 2/3, Batch number: 5, Cumulated accuracy: 0.59375\n",
      "Epoch 2/3, Batch number: 6, Cumulated accuracy: 0.5803571428571429\n",
      "Epoch 2/3, Batch number: 7, Cumulated accuracy: 0.58984375\n",
      "Epoch 2/3, Batch number: 8, Cumulated accuracy: 0.5972222222222222\n",
      "Epoch 2/3, Batch number: 9, Cumulated accuracy: 0.596875\n",
      "Epoch 2/3, Batch number: 10, Cumulated accuracy: 0.5767045454545454\n",
      "Epoch 2/3, Batch number: 11, Cumulated accuracy: 0.5859375\n",
      "Epoch 2/3, Batch number: 12, Cumulated accuracy: 0.5745192307692307\n",
      "Epoch 2/3, Batch number: 13, Cumulated accuracy: 0.578125\n",
      "Epoch 2/3, Batch number: 14, Cumulated accuracy: 0.5645833333333333\n",
      "Epoch 2/3, Batch number: 15, Cumulated accuracy: 0.56640625\n",
      "Epoch 2/3, Batch number: 16, Cumulated accuracy: 0.5588235294117647\n",
      "Epoch 2/3, Batch number: 17, Cumulated accuracy: 0.5607638888888888\n",
      "Epoch 2/3, Batch number: 18, Cumulated accuracy: 0.5592105263157895\n",
      "Epoch 2/3, Batch number: 19, Cumulated accuracy: 0.553125\n",
      "Epoch 2/3, Batch number: 20, Cumulated accuracy: 0.5580357142857143\n",
      "Epoch 2/3, Batch number: 21, Cumulated accuracy: 0.5625\n",
      "Epoch 2/3, Batch number: 22, Cumulated accuracy: 0.5570652173913043\n",
      "Epoch 2/3, Batch number: 23, Cumulated accuracy: 0.5611979166666666\n",
      "Epoch 2/3, Batch number: 24, Cumulated accuracy: 0.56\n",
      "Epoch 2/3, Batch number: 25, Cumulated accuracy: 0.5612980769230769\n",
      "Epoch 2/3, Batch number: 26, Cumulated accuracy: 0.5567129629629629\n",
      "Epoch 2/3, Batch number: 27, Cumulated accuracy: 0.5558035714285714\n",
      "Epoch 2/3, Batch number: 28, Cumulated accuracy: 0.5581896551724138\n",
      "Epoch 2/3, Batch number: 29, Cumulated accuracy: 0.55\n",
      "Epoch 2/3, Batch number: 30, Cumulated accuracy: 0.5534274193548387\n",
      "Epoch 2/3, Batch number: 31, Cumulated accuracy: 0.5556640625\n",
      "Epoch 2/3, Batch number: 32, Cumulated accuracy: 0.5568181818181818\n",
      "Epoch 2/3, Batch number: 33, Cumulated accuracy: 0.5606617647058824\n",
      "Epoch 2/3, Batch number: 34, Cumulated accuracy: 0.5642857142857143\n",
      "Epoch 2/3, Batch number: 35, Cumulated accuracy: 0.5590277777777778\n",
      "Epoch 2/3, Batch number: 36, Cumulated accuracy: 0.5548986486486487\n",
      "Epoch 2/3, Batch number: 37, Cumulated accuracy: 0.5567434210526315\n",
      "Epoch 2/3, Batch number: 38, Cumulated accuracy: 0.561698717948718\n",
      "Epoch 2/3, Batch number: 39, Cumulated accuracy: 0.56328125\n",
      "Epoch 2/3, Batch number: 40, Cumulated accuracy: 0.5617378048780488\n",
      "Epoch 2/3, Batch number: 41, Cumulated accuracy: 0.5626423690205011\n",
      "--- Epoch 2/3: Train loss: 0.6920, Train accuracy: 0.5626\n",
      "--- Epoch 2/3: Validation loss: 0.7459, Validation accuracy: 0.5061\n",
      "Epoch 3/3, Batch number: 0, Cumulated accuracy: 0.53125\n",
      "Epoch 3/3, Batch number: 1, Cumulated accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "model = CovidClassifer().to(device)\n",
    "\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies = train(model, train_dataloader, valid_dataloader, epochs = 3, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(train_losses, \"r-\", label = \"Train losses\")\n",
    "plt.plot(valid_losses, \"b-\", label = \"Validation losses\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(train_accuracies, \"r-\", label = \"Train accuracy\")\n",
    "plt.plot(valid_accuracies, \"b-\", label = \"Validation accuracy\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate(model, test_dataloader)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
